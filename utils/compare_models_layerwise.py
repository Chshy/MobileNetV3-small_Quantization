import torch
from collections import OrderedDict

def compare_models_layerwise(fp_model, quant_model, key_mapping, device='cuda', input_shape=(1, 3, 64, 64)):
    # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    fp_model.eval()
    quant_model.eval()
    fp_model.to(device)
    quant_model.to(device)

    
    # ç”Ÿæˆå›ºå®šè¾“å…¥æ•°æ®
    torch.manual_seed(42)
    input_data = torch.randn(*input_shape).to(device)
    
    # å­˜å‚¨å„å±‚è¾“å‡º
    fp_outputs = OrderedDict()
    quant_outputs = OrderedDict()

    # è·å–æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—
    fp_modules = dict(fp_model.named_modules())
    quant_modules = dict(quant_model.named_modules())

    # æ„å»ºæ¨¡å—æ˜ å°„å…³ç³»ï¼ˆå‚æ•°å -> æ¨¡å—å®ä¾‹ï¼‰
    module_mapping = {}
    for fp_param, quant_param in key_mapping.items():
        # æå–æ¨¡å—åç§°ï¼ˆå»æ‰å‚æ•°ç±»å‹åç¼€ï¼‰
        fp_module_name = '.'.join(fp_param.split('.')[:-1])
        quant_module_name = '.'.join(quant_param.split('.')[:-1])
        
        # è·å–å¯¹åº”æ¨¡å—å®ä¾‹
        if fp_module_name in fp_modules and quant_module_name in quant_modules:
            module_mapping[fp_modules[fp_module_name]] = quant_modules[quant_module_name]

    # æ³¨å†Œé’©å­çš„å‡½æ•°
    def register_hooks(model, output_dict, model_type):
        hooks = []
        for name, module in model.named_modules():
            def closure(m, name=name):
                def hook(module, input, output):
                    output_dict[f"{model_type}_{name}"] = output.detach()
                return hook
            hooks.append(module.register_forward_hook(closure(module)))
        return hooks

    # ä¸ºä¸¤ä¸ªæ¨¡å‹æ³¨å†Œé’©å­
    fp_hooks = register_hooks(fp_model, fp_outputs, "fp")
    quant_hooks = register_hooks(quant_model, quant_outputs, "quant")

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        _ = fp_model(input_data)
        _ = quant_model(input_data)

    # ç§»é™¤é’©å­
    for hook in fp_hooks + quant_hooks:
        hook.remove()

    # æ„å»ºåå‘æ˜ å°„ï¼šquantæ¨¡å—å -> fpæ¨¡å—å
    reverse_mapping = {v: k for k, v in module_mapping.items()}

    # é€å±‚å¯¹æ¯”è¾“å‡º
    mismatch_found = False
    for quant_name, quant_out in quant_outputs.items():
        # è·³è¿‡æ²¡æœ‰å¯¹åº”çš„å±‚
        if quant_name.split('_', 1)[1] not in ['.'.join(k.split('.')[:-1]) for k in key_mapping.values()]:
            continue

        # æ‰¾åˆ°å¯¹åº”çš„fpæ¨¡å—è¾“å‡º
        fp_module = reverse_mapping.get(quant_modules[quant_name.split('_', 1)[1]], None)
        if not fp_module:
            continue

        fp_name = f"fp_{list(fp_modules.keys())[list(fp_modules.values()).index(fp_module)]}"
        fp_out = fp_outputs.get(fp_name)

        if fp_out is None:
            print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”è¾“å‡ºï¼š{fp_name} -> {quant_name}")
            continue

        # å¯¹æ¯”å¼ é‡
        try:
            if not torch.allclose(fp_out, quant_out, atol=1e-5):
                print("\n" + "=" * 80)
                print(f"âŒ è¾“å‡ºä¸åŒ¹é…ï¼š\nFP: {fp_name}\nQuant: {quant_name}")
                print(f"FP shape: {fp_out.shape} | Quant shape: {quant_out.shape}")
                print(f"æœ€å¤§ç»å¯¹è¯¯å·®: {torch.max(torch.abs(fp_out - quant_out)).item():.5e}")
                print(f"å¹³å‡ç»å¯¹è¯¯å·®: {torch.mean(torch.abs(fp_out - quant_out)).item():.5e}")
                print("=" * 80 + "\n")
                mismatch_found = True
                # break  # å‘ç°ç¬¬ä¸€ä¸ªå·®å¼‚å³åœæ­¢
            else:
                print(f"âœ… åŒ¹é…æˆåŠŸï¼š{fp_name.ljust(50)} -> {quant_name}")
        except RuntimeError as e:
            print(f"â›” å½¢çŠ¶ä¸åŒ¹é…ï¼š{fp_name} ({fp_out.shape}) vs {quant_name} ({quant_out.shape})")
            mismatch_found = True
            break

    if not mismatch_found:
        print("\nğŸ‰ æ‰€æœ‰å¯¹åº”å±‚è¾“å‡ºå®Œå…¨åŒ¹é…ï¼")