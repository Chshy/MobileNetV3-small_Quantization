import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models._utils import _make_divisible

import math

class SqueezeExcitation(nn.Module):
    """Squeeze-and-Excitationæ¨¡å—"""

    def __init__(self, input_channels, squeeze_channels):
        super().__init__()

        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, kernel_size=1)
        self.activation = nn.ReLU(inplace = False)
        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, kernel_size=1)
        self.scale_activation = nn.Hardsigmoid(inplace = False)

    def forward(self, x):
        scale = self.avgpool(x)
        scale = self.fc1(scale)
        scale = self.activation(scale)
        scale = self.fc2(scale)
        scale = self.scale_activation(scale)
        return x * scale


class InvertedResidual(nn.Module):
    """InvertedResidualæ¨¡å—"""

    def __init__(self, 
                 in_channels,  # å·ç§¯1è¾“å…¥
                 out_channels, # å·ç§¯3è¾“å‡º
                 kernel_size,  # å·ç§¯2çš„kernel size
                 stride,       # å·ç§¯2çš„stride

                 # ä»¥ä¸‹å‚æ•°ç”¨äºè®¡ç®—å·ç§¯2çš„è¾“å…¥/è¾“å‡ºé€šé“æ•°
                 expansion_ratio = None,
                 hidden_dim = None, 

                 # ä»¥ä¸‹å‚æ•°ç”¨äºæ§åˆ¶SqueezeExcitationæ¨¡å—ä¸­é—´çš„é€šé“æ•°
                 use_se = True,
                 se_ratio = None,
                 
                 activation = nn.ReLU # æ¿€æ´»å‡½æ•°
                ):
        
        super().__init__()

        # è®¡ç®— hidden dim
        if hidden_dim is not None and expansion_ratio is not None:
            raise ValueError("ğŸš«Error: Only one of hidden_dim or expansion_ratio can be provided")
        if hidden_dim is None and expansion_ratio is None:
            raise ValueError("ğŸš«Error: Either hidden_dim or expansion_ratio must be provided")
        if hidden_dim is None: # ä½¿ç”¨ expansion_ratio è®¡ç®— hidden_dim
            hidden_dim = int(in_channels * expansion_ratio)

        # å½“ä¸”ä»…å½“ *è¾“å…¥è¾“å‡ºé€šé“/å°ºå¯¸åŒ¹é…* æ—¶, ä½¿ç”¨æ®‹å·®è¿æ¥
        self.use_residual = (stride == 1) and (in_channels == out_channels)
        
        # Expansion phase
        self.conv1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(hidden_dim)
        self.act1 = activation(inplace=False)
        
        # Depthwise convolution
        # å·ç§¯2éœ€è¦ä½¿ç”¨padding ä½¿å¾—è¾“å…¥å’Œè¾“å‡ºå°ºå¯¸ä¸€è‡´
        self.conv2 = nn.Conv2d(
            hidden_dim, hidden_dim, kernel_size, stride, 
            padding=kernel_size//2, groups=hidden_dim, bias=False
        )
        self.bn2 = nn.BatchNorm2d(hidden_dim)
        self.act2 = activation(inplace=False)
        
        # Squeeze-and-Excitation
        if use_se:
            if se_ratio is None:
                se_ratio = 0.25
                # SEä¸­é—´å±‚channel = SEè¾“å‡ºå±‚channel * se_ratio
                # è®ºæ–‡ç¬¬5.3èŠ‚, MobileNetV3å°†SEæ¨¡å—çš„ä¸­é—´é€šé“æ•°å›ºå®šä¸ºæ‰©å±•å±‚é€šé“æ•°çš„1/4
            squeeze_channel = _make_divisible(int(hidden_dim * se_ratio), 8) # è®¡ç®—SEä¸­é—´å±‚channel ç¡®ä¿squeeze_channelä¸º8çš„å€æ•°
            self.se = SqueezeExcitation(hidden_dim, squeeze_channel)
        else:
            self.se = None
        
        # Projection
        self.conv3 = nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        residual = x
        
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.act2(x)
        
        if self.se:
            x = self.se(x)
            
        x = self.conv3(x)
        x = self.bn3(x)
        
        if self.use_residual:
            x += residual
            
        return x

class MobileNetV3(nn.Module):
    """MobileNetV3 Base Class"""
    def __init__(self, config, num_classes=1000, dropout=0.8):
        super().__init__()
        layers = []
        
        # åˆ›å»ºå·ç§¯å—çš„è¾…åŠ©å‡½æ•°
        def build_conv_block(conv_cfg, in_ch):

            # åˆ›å»ºåŸºç¡€å·ç§¯å±‚
            block = [
                nn.Conv2d(
                    in_ch,
                    conv_cfg['out_channels'],
                    kernel_size=conv_cfg['kernel'],
                    stride=conv_cfg['stride'],
                    padding=conv_cfg['kernel'] // 2,
                    bias=False
                ),
                nn.BatchNorm2d(conv_cfg['out_channels']),
                nn.Hardswish(inplace=False) if conv_cfg.get('use_hs', False) else nn.ReLU(inplace=False)
            ]
            
            # æ·»åŠ SEæ¨¡å—ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if conv_cfg.get('use_se', False):
                se_ratio = conv_cfg.get('se_ratio', 0.25)
                # æ³¨æ„ï¼šSEçš„è¾“å…¥é€šé“åº”è¯¥æ˜¯å½“å‰å±‚çš„è¾“å‡ºé€šé“
                squeeze_channels = _make_divisible(
                    int(conv_cfg['out_channels'] * se_ratio), 8
                )
                block.append(SqueezeExcitation(
                    conv_cfg['out_channels'],  # ä¿®å¤åŸinit_convçš„é€šé“é”™è¯¯
                    squeeze_channels
                ))
            
            # è¿”å›é€šé“ä¿¡æ¯ç”¨äºåç»­å¤„ç†
            return block, conv_cfg['out_channels']
        
        # æ„å»ºåˆå§‹å·ç§¯å±‚
        init_block, in_channels = build_conv_block(
            config['init_conv'],
            config['input_channels']
        )
        layers += init_block
        
        # æ„å»ºä¸­é—´æ®‹å·®å—
        for block_cfg in config['blocks']:
            layers.append(InvertedResidual(
                in_channels=in_channels,
                out_channels=block_cfg['out_channels'],
                kernel_size=block_cfg['kernel'],
                stride=block_cfg['stride'],
                expansion_ratio=block_cfg.get('expansion', None),
                hidden_dim=block_cfg.get('exp_size', None),
                use_se=block_cfg.get('use_se', False),
                se_ratio=block_cfg.get('se_ratio', 0.25),
                activation=nn.Hardswish if block_cfg.get('use_hs', False) else nn.ReLU
            ))
            in_channels = block_cfg['out_channels']
        
        # æ„å»ºæœ€ç»ˆå·ç§¯å±‚
        final_block, final_out_ch = build_conv_block(
            config['final_conv'],
            in_channels
        )
        layers += final_block
        
        self.features = nn.Sequential(*layers)
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        
        self.classifier = nn.Sequential(
            nn.Linear(final_out_ch, config.get('classifier_hidden_dim', 1280)),
            nn.Hardswish(inplace = False),
            nn.Dropout(p=dropout),
            nn.Linear(config.get('classifier_hidden_dim', 1280), num_classes)
        )
        
    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = x.flatten(1)
        x = self.classifier(x)
        return x


# é¢„å®šä¹‰é…ç½®
def mobilenet_v3_large(num_classes=10):
    config = {
        'input_channels': 3, # è¾“å…¥å›¾ç‰‡çš„é€šé“æ•°
        'init_conv': {'kernel': 3, 'out_channels': 16, 'use_se': False, 'use_hs': True, 'stride': 2},
        'blocks': [
            {'kernel': 3, 'exp_size':16, 'out_channels': 16, 'use_se': False, 'use_hs': False, 'stride': 1},
            {'kernel': 3, 'exp_size':64, 'out_channels': 24, 'use_se': False, 'use_hs': False, 'stride': 2},
            {'kernel': 3, 'exp_size':72, 'out_channels': 24, 'use_se': False, 'use_hs': False, 'stride': 1},
            {'kernel': 5, 'exp_size':72, 'out_channels': 40, 'use_se': True, 'use_hs': False, 'stride': 2},
            {'kernel': 5, 'exp_size':120, 'out_channels': 40, 'use_se': True, 'use_hs': False, 'stride': 1},
            {'kernel': 5, 'exp_size':120, 'out_channels': 40, 'use_se': True, 'use_hs': False, 'stride': 1},
            {'kernel': 3, 'exp_size':240, 'out_channels': 80, 'use_se': False, 'use_hs': True, 'stride': 2},
            {'kernel': 3, 'exp_size':200, 'out_channels': 80, 'use_se': False, 'use_hs': True, 'stride': 1},
            {'kernel': 3, 'exp_size':184, 'out_channels': 80, 'use_se': False, 'use_hs': True, 'stride': 1},
            {'kernel': 3, 'exp_size':184, 'out_channels': 80, 'use_se': False, 'use_hs': True, 'stride': 1},
            {'kernel': 3, 'exp_size':480, 'out_channels': 112, 'use_se': True, 'use_hs': True, 'stride': 1},
            {'kernel': 3, 'exp_size':672, 'out_channels': 112, 'use_se': True, 'use_hs': True, 'stride': 1},
            {'kernel': 5, 'exp_size':672, 'out_channels': 160, 'use_se': True, 'use_hs': True, 'stride': 2},
            {'kernel': 5, 'exp_size':960, 'out_channels': 160, 'use_se': True, 'use_hs': True, 'stride': 1},
            {'kernel': 5, 'exp_size':960, 'out_channels': 160, 'use_se': True, 'use_hs': True, 'stride': 1},
        ],
        'final_conv': {'kernel': 1, 'out_channels': 960, 'use_se': False, 'use_hs': True, 'stride': 1},
        'classifier_hidden_dim': 1280
    }
    return MobileNetV3(config, num_classes = num_classes)

def mobilenet_v3_small(num_classes=10):
    config = {
        'input_channels': 3,
        'init_conv': {'kernel': 3, 'out_channels': 16, 'use_se': False, 'use_hs': True, 'stride': 2},
        'blocks': [
            {'kernel': 3, 'exp_size':16, 'out_channels': 16, 'use_se': True, 'use_hs': False, 'stride': 2},
            {'kernel': 3, 'exp_size':72, 'out_channels': 24, 'use_se': False, 'use_hs': False, 'stride': 2},
            {'kernel': 3, 'exp_size':88, 'out_channels': 24, 'use_se': False, 'use_hs': False, 'stride': 1},
            {'kernel': 5, 'exp_size':96, 'out_channels': 40, 'use_se': True, 'use_hs': True, 'stride': 2},
            {'kernel': 5, 'exp_size':240, 'out_channels': 40, 'use_se': True, 'use_hs': True, 'stride': 1},
            {'kernel': 5, 'exp_size':240, 'out_channels': 40, 'use_se': True, 'use_hs': True, 'stride': 1},
            {'kernel': 5, 'exp_size':120, 'out_channels': 48, 'use_se': True, 'use_hs': True, 'stride': 1},
            {'kernel': 5, 'exp_size':144, 'out_channels': 48, 'use_se': True, 'use_hs': True, 'stride': 1},
            {'kernel': 5, 'exp_size':288, 'out_channels': 96, 'use_se': True, 'use_hs': True, 'stride': 2},
            {'kernel': 5, 'exp_size':576, 'out_channels': 96, 'use_se': True, 'use_hs': True, 'stride': 1},
            {'kernel': 5, 'exp_size':576, 'out_channels': 96, 'use_se': True, 'use_hs': True, 'stride': 1},
        ],
        'final_conv': {'kernel': 1, 'out_channels': 576, 'use_se': True, 'use_hs': True, 'stride': 1},
        'classifier_hidden_dim': 1024
    }
    return MobileNetV3(config, num_classes = num_classes)

# ç¤ºä¾‹ç”¨æ³•
if __name__ == '__main__':
    model_large = mobilenet_v3_large()
    model_small = mobilenet_v3_small()
    
    # è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹
    custom_config = {
        'input_channels': 3,
        'init_conv': {'kernel': 3, 'out_channels': 24, 'use_se': False, 'use_hs': False, 'stride': 2},
        'blocks': [
            {'kernel': 3, 'expansion':2, 'out_channels': 32, 'use_se': True, 'use_hs': False, 'stride': 1},
            {'kernel': 5, 'expansion':4, 'out_channels': 64, 'use_se': False, 'use_hs': False, 'stride': 2},
        ],
        'final_conv': {'kernel': 1, 'out_channels': 512, 'use_se': True, 'use_hs': True, 'stride': 1},
        'classifier_hidden_dim': 1024
    }
    model_custom = MobileNetV3(custom_config, num_classes=10)