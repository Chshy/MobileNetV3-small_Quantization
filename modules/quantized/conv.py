import os
import sys
# current_script_path = os.path.abspath(__file__)
# project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_script_path)))
# sys.path.insert(0, project_root)

# # from quant_modules.basic.quantize import quantize, dequantize
# # from quant_modules.basic.pseudo_quantize import pseudo_quantize
# from quant_modules.basic.calib import QuantCalibrator

from .calib import QuantCalibrator

import torch
import torch.nn as nn
import torch.nn.functional as F

import warnings


# æ ¡å‡†æ¨¡å¼: æµ®ç‚¹æ•°è¿ç®—ï¼Œç»Ÿè®¡è¾“å…¥å’Œè¾“å‡º
#           æƒé‡æ˜¯å›ºå®šçš„ ä¸éœ€è¦æ•°æ®å°±èƒ½ç»Ÿè®¡
# QATæ¨¡å¼: ä¼ªé‡åŒ–ç”Ÿæ•ˆï¼Œç»Ÿè®¡è¾“å…¥å’Œè¾“å‡º
#          æƒé‡ä¸ä½¿ç”¨emaç»Ÿè®¡

# class QuantNode(nn.Module):
#     def __init__(self, act_num_bits=8, act_symmetric=True, act_signed=True):
#         super().__init__()

#         self.act_quant = QuantCalibrator(num_bits=act_num_bits, symmetric=act_symmetric, signed=act_signed, enable_ema=True)
        
#         self.enable_quant = True
#         self.enable_calib = True
    
#     def process_result(self, result):
#         if self.enable_calib:
#             self.act_quant.calibrate(result)
#         if self.enable_quant:
#             proc_result = self.act_quant.quantize(result)
#         else:
#             proc_result = result
#         return proc_result
    
#     def forward(self, x):
#         return self.process_result(x)



class BaseQuantConv2d(nn.Conv2d):
    def __init__(self, *args, 
                 weight_num_bits=8, 
                 weight_symmetric=True,
                 weight_signed=True,
                 bias_num_bits=8, 
                 bias_symmetric=True,
                 bias_signed=True,
                 act_num_bits=8, 
                 act_symmetric=False,
                 act_signed=True,
                 init_conv=None, 
                 **kwargs):
        
        if init_conv is not None:

            # æ£€æŸ¥æ˜¯å¦ä¼ å…¥äº†å…¶ä»–å‚æ•°ï¼Œå¦‚æœæœ‰åˆ™æŠ¥é”™
            if len(args) > 0 or len(kwargs) > 0:
                raise ValueError("ğŸš«Error: å½“ä½¿ç”¨ 'init_conv' åˆå§‹åŒ–æ—¶, ä¸å¯ä¼ å…¥å…¶ä»–å‚æ•°")
            
            params = {
                'in_channels': init_conv.in_channels,
                'out_channels': init_conv.out_channels,
                'kernel_size': init_conv.kernel_size,
                'stride': init_conv.stride,
                'padding': init_conv.padding,
                'dilation': init_conv.dilation,
                'groups': init_conv.groups,
                'bias': init_conv.bias is not None,
                'padding_mode': init_conv.padding_mode
            }
            super().__init__(**params)
            # self.weight.data.copy_(init_conv.weight.data)
            # if params['bias']:
            #     self.bias.data.copy_(init_conv.bias.data)
            self.load_state_dict(init_conv.state_dict())
        else:
            # è‹¥æ—  init_conv åˆ™ä½¿ç”¨é»˜è®¤æ–¹æ³•åˆå§‹åŒ–
            super().__init__(*args, **kwargs)
        
        # åˆå§‹åŒ–é‡åŒ–å™¨
        self.weight_quant = QuantCalibrator(num_bits=weight_num_bits, symmetric=weight_symmetric, signed=weight_signed, enable_ema=False)
        if self.bias is not None:
            self.bias_quant = QuantCalibrator(num_bits=bias_num_bits, symmetric=bias_symmetric, signed=bias_signed, enable_ema=False)
        self.act_quant = QuantCalibrator(num_bits=act_num_bits, symmetric=act_symmetric, signed=act_signed, enable_ema=True)
        
        self.enable_quant = True
        self.enable_calib = True

    def process_weight(self):
        # æ ¡å‡†å¹¶é‡åŒ–æƒé‡å’Œåç½®
        if self.enable_calib:
            self.weight_quant.calibrate(self.weight)
            if self.bias is not None:
                self.bias_quant.calibrate(self.bias)

        if self.enable_quant:
            proc_weight = self.weight_quant.quantize(self.weight)
            if self.bias is not None:
                proc_bias = self.bias_quant.quantize(self.bias)
            else:
                proc_bias = None
        else:
            proc_weight = self.weight
            proc_bias = self.bias

        return proc_weight, proc_bias
       
    def process_result(self, result):
        # æ ¡å‡†å¹¶é‡åŒ–è¾“å‡º
        if self.enable_calib:
            self.act_quant.calibrate(result)
        if self.enable_quant:
            proc_result = self.act_quant.quantize(result)
        else:
            proc_result = result
        return proc_result

class QuantConv2d(BaseQuantConv2d):
    def __init__(self, *args, 
                 act_num_bits=8, 
                 act_symmetric=True,  # é»˜è®¤è¾“å‡ºå¯¹ç§°é‡åŒ–
                 **kwargs):
        """
        åˆå§‹åŒ–æ–¹æ³•ï¼š
        - args/kwargs: å·ç§¯å±‚å‚æ•°
        - init_conv: ç”¨äºåˆå§‹åŒ–çš„ç°æœ‰å·ç§¯å±‚ï¼ˆå¯é€‰ï¼‰
        """
        super().__init__(*args, act_num_bits=act_num_bits, act_symmetric=act_symmetric, **kwargs)

    def forward(self, x):
        proc_weight, proc_bias = self.process_weight()
        result = F.conv2d(x, proc_weight, proc_bias, self.stride, self.padding, self.dilation, self.groups)
        return self.process_result(result)

class QuantConv2dAct(BaseQuantConv2d):
    def __init__(self, *args, 
                 activation=nn.ReLU(), 
                 act_num_bits=8,
                 act_symmetric=True,  # é»˜è®¤è¾“å‡ºå¯¹ç§°é‡åŒ–
                 **kwargs):
        """
        åˆå§‹åŒ–æ–¹æ³•ï¼š
        - args/kwargs: å·ç§¯å±‚å‚æ•°
        - init_conv: ç”¨äºåˆå§‹åŒ–çš„ç°æœ‰å·ç§¯å±‚ï¼ˆå¯é€‰ï¼‰
        - activation: æ¿€æ´»å‡½æ•°ï¼ˆé»˜è®¤ReLUï¼‰
        """
        super().__init__(*args, act_num_bits=act_num_bits, act_symmetric=act_symmetric, **kwargs)
        self.activation = activation

    def forward(self, x):
        proc_weight, proc_bias = self.process_weight()
        result = F.conv2d(x, proc_weight, proc_bias, self.stride, self.padding, self.dilation, self.groups)
        if self.activation is not None:
            result = self.activation(result)
        return self.process_result(result)
    
class QuantConv2dBnAct(BaseQuantConv2d):
    def __init__(self, *args, 
                 init_conv=None, 
                 init_bn=None, 
                 activation=nn.ReLU(),
                 act_num_bits=8,
                 act_symmetric=False,  # é»˜è®¤è¾“å‡ºéå¯¹ç§°é‡åŒ–
                 **kwargs):
        """
        åˆå§‹åŒ–æ–¹æ³•ï¼š
        - args/kwargs: å·ç§¯å±‚å‚æ•°
        - init_conv: ç”¨äºåˆå§‹åŒ–çš„ç°æœ‰å·ç§¯å±‚ï¼ˆå¯é€‰ï¼‰
        - init_bn: ç”¨äºåˆå§‹åŒ–çš„ç°æœ‰æ‰¹å½’ä¸€åŒ–å±‚ï¼ˆå¯é€‰ï¼‰
        - activation: æ¿€æ´»å‡½æ•°ï¼ˆé»˜è®¤ReLUï¼‰
        """
        super().__init__(*args, act_num_bits=act_num_bits, act_symmetric=act_symmetric, init_conv=init_conv, **kwargs)
        
        if self.bias is not None:
            warnings.warn("âš ï¸Warning: å½“ä½¿ç”¨ BatchNorm æ—¶, å·ç§¯å±‚ä¸åº”åŒ…å«åç½®, BatchNorm ä¼šæŠµæ¶ˆåç½®çš„ä½œç”¨", UserWarning)
        
        # åˆå§‹åŒ–BatchNormå±‚
        self.bn = nn.BatchNorm2d(self.out_channels)
        if init_bn is not None:
            if init_bn.num_features != self.out_channels: # éªŒè¯å·ç§¯çš„è¾“å‡ºé€šé“æ•°å’Œ bn çš„é€šé“æ•°åŒ¹é…
                raise ValueError(f"ğŸš«Error: BatchNormé€šé“æ•°({init_bn.num_features})ä¸å·ç§¯è¾“å‡ºé€šé“({self.out_channels})ä¸åŒ¹é…")
            self.bn.load_state_dict(init_bn.state_dict())
        
        self.activation = activation

        # debuf
        # print(f"BaseQuantConv2d: in_channels={self.in_channels}, out_channels={self.out_channels}, kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding}, dilation={self.dilation}, groups={self.groups}, bias={self.bias is not None}, bn={self.bn is not None}, activation={self.activation is not None}")
        
        bias = getattr(self, "bias_quant", None)
        bias_bits = bias.num_bits if bias else "N/A"
        bias_sym = "T" if bias and bias.symmetric else "N"
        bias_sign = "S" if bias and bias.signed else "N"

        print(
            f"BaseQuantConv2d: W: {self.weight_quant.num_bits} "
            f"{'T' if self.weight_quant.symmetric else 'F'} "
            f"{'S' if self.weight_quant.signed else 'U'}, "
            
            f"B: {bias_bits} {bias_sym} {bias_sign}, "
            
            f"A: {self.act_quant.num_bits} "
            f"{'T' if self.act_quant.symmetric else 'F'} "
            f"{'S' if self.act_quant.signed else 'U'}"
        )



    def forward(self, x):
        proc_weight, proc_bias = self.process_weight()
        result = F.conv2d(x, proc_weight, proc_bias, self.stride, self.padding, self.dilation, self.groups)
        result = self.bn(result)
        if self.activation is not None:
            result = self.activation(result)
        return self.process_result(result)
